# ========================================================
# 1. Import libraries
# ========================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    median_absolute_error
)
from sklearn.tree import DecisionTreeRegressor
from scipy.stats import pearsonr


# ========================================================
# 2. Load dataset and select features
# ========================================================
data = pd.read_excel('/content/Database.xlsx')

f = [
    "fc' (MPa)",
    "fyl  (MPa)",
    "ρl",
    "Vol.ρt",
    "h (mm) Depth",
    "L (mm)",
    "cover (mm)",
    "Axial load ratio"
]

X, Y = data[f], data['Collapse']


# ========================================================
# 3. Shuffle data for randomness
# ========================================================
np.random.seed(6)
p = np.random.permutation(len(X))

X, Y, data = (
    X.iloc[p].reset_index(drop=True),
    Y.iloc[p].reset_index(drop=True),
    data.iloc[p].reset_index(drop=True)
)

print(pd.DataFrame(X, columns=f).head(10))
print(Y.head(10))


# ========================================================
# 4. Feature scaling and selection
# ========================================================
X = StandardScaler().fit_transform(X)
X = SelectKBest(f_regression, k='all').fit_transform(X, Y)


# ========================================================
# 5. Initialize variables for DT modeling
# ========================================================
best_dt_model = None
grid_search = None
X_train = X_test = Y_train = Y_test = None


# ========================================================
# 6. Split data, GridSearchCV, and model training
# ========================================================
test_data_prediction = (
    lambda X, Y: (
        lambda Xs, Ys, m, gs, Xt, Xs_, Yt, Ys_: (
            globals().__setitem__('_a1', m) or
            globals().__setitem__('_b2', gs) or
            globals().__setitem__('_c3', Xt) or
            globals().__setitem__('_d4', Xs_) or
            globals().__setitem__('_e5', Yt) or
            globals().__setitem__('_f6', Ys_) or
            m.predict(Xs)
        )
    )(
        *(
            lambda *q: (
                q[1], q[3],
                (
                    lambda __model:
                        (
                            lambda __pack:
                                next(
                                    map(
                                        lambda _: __model.fit(*__pack),
                                        (None,)
                                    ),
                                    __model
                                )
                        )(
                            tuple(
                                map(
                                    np.copy,
                                    (
                                        np.concatenate((q[0], q[1])),
                                        np.concatenate((q[2], q[3]))
                                    )
                                )
                            )
                        )
                )(
                    (lambda cls, prm: cls(**prm))(
                        type(q[4].best_estimator_),
                        q[4].best_estimator_.get_params()
                    )
                ),
                q[4],
                q[0], q[1], q[2], q[3]
            )
        )(
            *(
                lambda Xt, Xs, Yt, Ys: (
                    Xt, Xs, Yt, Ys,
                    GridSearchCV(
                        DecisionTreeRegressor(),
                        {
                            'max_depth': np.arange(12, 21),
                            'min_samples_split': np.arange(12, 21)
                        },
                        cv=5,
                        scoring='neg_mean_absolute_error'
                    ).fit(Xt, Yt)
                )
            )(*train_test_split(X, Y, test_size=0.2, random_state=6))
        )
    )
)(X, Y)


# ========================================================
# 7. Predictions
# ========================================================
Y_train_np = np.array(_e5).ravel()
Y_test_np  = np.array(_f6).ravel()

trp = _a1.predict(_c3)
tep = _a1.predict(_d4)

trp_np = np.array(trp).ravel()
tep_np = np.array(tep).ravel()


# ========================================================
# 7.1 Training and Testing Sample Counts
# ========================================================
num_train = len(Y_train_np)
num_test  = len(Y_test_np)

print("Number of Training Samples (80%):", num_train)
print("Number of Testing Samples (20%):", num_test)

print("\nTraining Set: Actual vs Predicted")
for i in range(num_train):
    print(
        f"Sample {i+1}: Actual = {Y_train_np[i]:.4f}, "
        f"Predicted = {trp_np[i]:.4f}"
    )

print("\nTesting Set: Actual vs Predicted")
for i in range(num_test):
    print(
        f"Sample {i+1}: Actual = {Y_test_np[i]:.4f}, "
        f"Predicted = {tep_np[i]:.4f}"
    )


# ========================================================
# 8. Prediction errors
# ========================================================
errs_np  = np.abs(tep_np - Y_test_np)
res_np   = Y_test_np - tep_np

trerr_np = np.abs(Y_train_np - trp_np) / Y_train_np * 100
teerr_np = np.abs(Y_test_np - tep_np) / Y_test_np * 100


# ========================================================
# 9. Plot results
# ========================================================
plt.figure(figsize=(10, 6))
plt.scatter(tep_np, Y_test_np)
plt.title('Scatter plot of Actual vs Predicted Drift (DT)')
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(Y_test_np, marker='o', linestyle='-', label='Actual Value')
plt.plot(tep_np, marker='o', linestyle='-', label='Predicted Value')
plt.title('Actual Drift vs Predicted Drift')
plt.xlabel('Sample Index')
plt.ylabel('Drift')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(errs_np, bins=30, edgecolor='k')
plt.xlabel("Prediction Error")
plt.ylabel("Frequency")
plt.title("Histogram of Prediction Errors")
plt.grid(True)
plt.show()


# ========================================================
# 10. Advanced scatter with marginal histograms
# ========================================================
fig = plt.figure(figsize=(10, 10))
gs = plt.GridSpec(4, 4, hspace=.4, wspace=.4)

ax_s  = fig.add_subplot(gs[1:4, 0:3])
ax_ht = fig.add_subplot(gs[0, 0:3], sharex=ax_s)
ax_hr = fig.add_subplot(gs[1:4, 3], sharey=ax_s)

ax_s.scatter(Y_train_np, trp_np, alpha=.8, color='b', marker='o', label='Training Set (Circle)')
ax_s.scatter(Y_test_np, tep_np, alpha=.8, color='r', marker='*', label='Testing Set (Star)')

ax_s.plot(
    [min(Y_test_np), max(Y_test_np)],
    [min(Y_test_np), max(Y_test_np)],
    'k--'
)

for a, b in [(1.3, 'g--'), (1.6, 'orange'), (0.7, 'g--'), (0.4, 'orange')]:
    ax_s.plot(
        [min(Y_test_np), max(Y_test_np)],
        [min(Y_test_np) * a, max(Y_test_np) * a],
        b
    )

ax_s.set_xlabel("Observed", fontsize=18)
ax_s.set_ylabel("Predicted", fontsize=18)
ax_s.legend(fontsize=12)
ax_s.grid(True)

ax_ht.hist(Y_train_np, bins=15, color='b', alpha=.6, edgecolor='k')
ax_ht.hist(Y_test_np, bins=15, color='r', alpha=.6, edgecolor='k')
ax_ht.set_ylabel("Frequency")
ax_ht.legend(['Training', 'Testing'])

ax_hr.hist(trp_np, bins=15, orientation='horizontal', color='b', alpha=.6, edgecolor='k')
ax_hr.hist(tep_np, bins=15, orientation='horizontal', color='r', alpha=.6, edgecolor='k')
ax_hr.set_xlabel("Frequency")
ax_hr.legend(['Training', 'Testing'])

plt.setp(ax_ht.get_xticklabels(), visible=False)
plt.setp(ax_hr.get_yticklabels(), visible=False)
plt.show()


# ========================================================
# 11. Evaluation metrics
# ========================================================
y = Y_test_np
t = tep_np

errs = np.abs(t - y)

rmse   = np.sqrt(mean_squared_error(y, t))
r2     = r2_score(y, t)
p, _   = pearsonr(y, t)
mape   = np.mean(errs / y * 100)
acc    = 100 - mape
mae    = mean_absolute_error(y, t)
theil  = (
    np.sqrt(np.mean((t - y) ** 2)) /
    (np.sqrt(np.mean(y ** 2)) + np.sqrt(np.mean(t ** 2)))
)
medae  = median_absolute_error(y, t)
logmse = mean_squared_error(np.log1p(y), np.log1p(t))

print("RMSE:", rmse, "R2:", r2, "Pearson:", p)
print("Accuracy:", round(acc, 2), "%", "MAPE:", round(mape, 2), "%")
print(
    "MAE:", mae,
    "Theil U:", theil,
    "MedAE:", medae,
    "Log-MSE:", logmse
)
print("Best Hyperparameters:", _b2.best_params_)
